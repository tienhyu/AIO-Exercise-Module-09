{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paIT4EzF76f9"
      },
      "source": [
        "# **1.1. Install Selenium library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHMlACOocOcp",
        "outputId": "cf00599a-2862-4a5e-b3a7-e6984e3d17d1"
      },
      "outputs": [],
      "source": [
        "# %%shell\n",
        "# # Ubuntu no longer distributes chromium-browser outside of snap\n",
        "# #\n",
        "# # Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n",
        "\n",
        "# # Add debian buster\n",
        "# cat > /etc/apt/sources.list.d/debian.list << \"EOF\"\n",
        "# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "# EOF\n",
        "\n",
        "# # Add keys\n",
        "# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "# apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "# apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "# apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# # Prefer debian repo for chromium* packages only\n",
        "# # Note the double-blank lines between entries\n",
        "# cat > /etc/apt/preferences.d/chromium.pref << \"EOF\"\n",
        "# Package: *\n",
        "# Pin: release a=eoan\n",
        "# Pin-Priority: 500\n",
        "\n",
        "\n",
        "# Package: *\n",
        "# Pin: origin \"deb.debian.org\"\n",
        "# Pin-Priority: 300\n",
        "\n",
        "\n",
        "# Package: chromium*\n",
        "# Pin: origin \"deb.debian.org\"\n",
        "# Pin-Priority: 700\n",
        "# EOF\n",
        "\n",
        "# # Install chromium and chromium-driver\n",
        "# apt-get update\n",
        "# apt-get install chromium chromium-driver\n",
        "\n",
        "# # Install selenium\n",
        "# pip install selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LT6D4Jx76gB"
      },
      "source": [
        "# **1.2. Import the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6KzV8rp2UT7Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "import pandas as pd\n",
        "import random\n",
        "import hashlib\n",
        "import urllib.parse\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci8XX5XK76gB"
      },
      "source": [
        "# **1.3. Initialize web browser driver**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mxnabkxrUWZz"
      },
      "outputs": [],
      "source": [
        "WEBDRIVER_DELAY_TIME_INT = 20\n",
        "TIMEOUT_INT = 20\n",
        "service = Service(ChromeDriverManager().install())\n",
        "# service = Service(executable_path=r\"/usr/bin/chromedriver\")\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"window-size=1920x1080\")\n",
        "chrome_options.headless = True\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "driver.implicitly_wait(TIMEOUT_INT)\n",
        "wait = WebDriverWait(driver, WEBDRIVER_DELAY_TIME_INT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NAa_BMq76gC"
      },
      "source": [
        "# **1.4. Build a function to extract image paths from a web page**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZLWnD3FjUZQf"
      },
      "outputs": [],
      "source": [
        "def get_image_links_from_page(page_url, driver):\n",
        "    driver.get(page_url)\n",
        "    try:\n",
        "        container = wait.until(EC.presence_of_element_located(\n",
        "            (By.CSS_SELECTOR, \"div.FS5UE28h.container\")\n",
        "        ))\n",
        "        image_items = wait.until(EC.presence_of_all_elements_located(\n",
        "            (By.CSS_SELECTOR, \"div.LQY5mtmC div.aLnnpRah.text-center\"))\n",
        "        )\n",
        "\n",
        "        image_links = []\n",
        "        for img_elem in image_items:\n",
        "            img_div = img_elem.find_element(By.CSS_SELECTOR, \"div.Mw1EAtrx img, img\")\n",
        "\n",
        "            img_url = img_div.get_attribute(\"src\")\n",
        "            img_title = img_div.get_attribute(\"title\")\n",
        "            if img_url:\n",
        "                image_links.append((img_url, img_title))\n",
        "\n",
        "        return image_links\n",
        "    except Exception as e:\n",
        "        print(f\"Error while trying to extract images: {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZTWKZUt76gC"
      },
      "source": [
        "# **1.5. Build a function to check for duplicate images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fgopVhAK76gC"
      },
      "outputs": [],
      "source": [
        "def hash_image_content(url):\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            return hashlib.md5(response.content).hexdigest()\n",
        "        else:\n",
        "            print(f\"Error downloading image from {url}; status: {response.status_code}\")\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error with the image download for {url}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOowRUiw76gD"
      },
      "source": [
        "# **1.6. Build a function to convert image format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o33Ztj0F76gD"
      },
      "outputs": [],
      "source": [
        "def convert_webp_to_jpg(webp_data):\n",
        "    try:\n",
        "        img = Image.open(BytesIO(webp_data))\n",
        "        if img.format == 'WEBP':\n",
        "            if img.mode == 'RGBA':\n",
        "                img = img.convert('RGB')\n",
        "            buffer = BytesIO()\n",
        "            img.save(buffer, format=\"JPEG\")\n",
        "            return buffer.getvalue()\n",
        "        else:\n",
        "            return webp_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting WebP to JPG: {e}\")\n",
        "        return webp_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jDhmCyK76gD"
      },
      "source": [
        "# **1.7. Build image loading function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P9AoFwcq76gD"
      },
      "outputs": [],
      "source": [
        "def download_image(img_url, img_name, folder_path):\n",
        "    try:\n",
        "        response = requests.get(img_url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            img_path = os.path.join(folder_path, f\"{img_name}\")\n",
        "            img_data = convert_webp_to_jpg(response.content)\n",
        "            with open(img_path, \"wb\") as f:\n",
        "                f.write(img_data)\n",
        "        else:\n",
        "            print(f\"Error downloading image from {img_url}; status: {response.status_code}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error with the image download for {img_url}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2qYzBp876gD"
      },
      "source": [
        "# **1.8. Build a function to collect information of an image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xYzkjqBQ76gD"
      },
      "outputs": [],
      "source": [
        "def process_image_page(image_url, img_title, folder_path, idx, tag, seen_hashes):\n",
        "    img_hash = hash_image_content(image_url)\n",
        "    if img_hash and img_hash not in seen_hashes:\n",
        "        seen_hashes.add(img_hash)\n",
        "        new_file_name = f\"{tag}_{idx:07d}.jpg\"\n",
        "        download_image(image_url, new_file_name, folder_path)\n",
        "        metadata = {\n",
        "            \"file_name\": new_file_name,\n",
        "            \"image_url\": image_url,\n",
        "            \"image_title\": img_title,\n",
        "            \"tag\": tag\n",
        "        }\n",
        "        return metadata\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D3xu9qf76gD"
      },
      "source": [
        "# **1.9. Build a function to browse through each web page**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S_ebtkYu76gD"
      },
      "outputs": [],
      "source": [
        "def loop_over_pages(base_url, tags, total_pages, driver, folder_path):\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "    all_metadata = []\n",
        "    seen_hashes = set()\n",
        "\n",
        "    for tag in tags:\n",
        "        all_images = []\n",
        "\n",
        "        for page in tqdm(range(1, total_pages + 1), desc=f\"Extracting Images for {tag}\", unit=\"page\"):\n",
        "            page_url = f\"{base_url}/emoji-list/tag/{tag}?page={page}\"\n",
        "            images = get_image_links_from_page(page_url, driver)\n",
        "            all_images.extend(images)\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        metadata_list = []\n",
        "        for idx, (img_url, img_title) in enumerate(all_images, start=1):\n",
        "            metadata = process_image_page(img_url, img_title, folder_path, idx, tag, seen_hashes)\n",
        "            if metadata:\n",
        "                metadata_list.append(metadata)\n",
        "\n",
        "        all_metadata.extend(metadata_list)\n",
        "\n",
        "    return all_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-wyEuhe76gE"
      },
      "source": [
        "# **1.10. Build a function to save metadata information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PKwaZKQg76gE"
      },
      "outputs": [],
      "source": [
        "def save_metadata(metadata_list, metadata_file):\n",
        "    df = pd.DataFrame(metadata_list)\n",
        "    df.to_csv(metadata_file, index=False, encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTiKALJl76gE"
      },
      "source": [
        "# **1.11. Perform data collection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekgTWMoIItRB",
        "outputId": "a31d14a5-48ed-4f04-d662-13c471712f58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Images for Panda: 100%|██████████| 1000/1000 [41:29<00:00,  2.49s/page]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start downloading images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Images: 100%|██████████| 3866/3866 [23:55<00:00,  2.69image/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download images completed.\n",
            "Total crawled images: 3930.\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(\"crawled_data\", exist_ok=True)\n",
        "folder_path = os.path.join(\"crawled_data\", \"images\")\n",
        "metadata_file = os.path.join(\"crawled_data\", \"metadata.csv\")\n",
        "\n",
        "base_url = \"https://discords.com\"\n",
        "tags = [\"Panda\"]\n",
        "total_pages = 1000\n",
        "\n",
        "metadata_list = loop_over_pages(base_url, tags, total_pages, driver, folder_path)\n",
        "save_metadata(metadata_list, metadata_file)\n",
        "\n",
        "print(\"Start downloading images...\")\n",
        "with tqdm(total=len(metadata_list), desc=\"Downloading Images\", unit=\"image\") as pbar:\n",
        "    for metadata in metadata_list:\n",
        "        img_url = metadata['image_url']\n",
        "        file_name = metadata['file_name']\n",
        "        download_image(img_url, file_name, folder_path)\n",
        "        pbar.update(1)\n",
        "\n",
        "print(\"Download images completed.\")\n",
        "\n",
        "total_crawled_images = len(os.listdir(folder_path))\n",
        "print(f\"Total crawled images: {total_crawled_images}.\")\n",
        "\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P9azTONGUZVU"
      },
      "outputs": [],
      "source": [
        "# !zip -r crawled_panda_emojis_1000_pages.zip crawled_data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Kaggle31012",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
